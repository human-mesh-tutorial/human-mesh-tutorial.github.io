
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="CVPR'26 Tutorial: 3D Human Mesh Modeling and Recovery from RGB and LiDAR">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="Human Mesh Modeling Tutorial">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Humans: Face, body, pose, gesture, movement, 3D from single images, 3D from multi-view and sensors, Robot perception, Autonomous driving, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="Romain Brégier, István Sárándi, Salma Galaaoui">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="NAVER LABS Europs, valeo.ai, Tubingen AI Center">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="3D Human Mesh Modeling and Recovery from RGB and LiDAR">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="Human Mesh Modeling Tutorial">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://human-mesh-tutorial.github.io>
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="3D Human Mesh Modeling and Recovery from RGB and LiDAR - Preview">
  <meta property="article:published_time" content="2026-02-01T00:00:00.000Z">
  <meta property="article:author" content="Romain Brégier">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="human pose">
  <meta property="article:tag" content="3D CV">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="CVPR'26 Tutorial: 3D Human Mesh Modeling and Recovery from RGB and LiDAR">
  <meta name="citation_author" content="Brégier, Romain">
  <meta name="citation_author" content="Sarandi, Istvan">
  <meta name="citation_author" content="Galaaoui, Salma">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="CVPR 2026">
  <!-- <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf"> -->
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>3D Human Mesh Modeling and Recovery from RGB and LiDAR | CVPR'26 Tutorial</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <!-- <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script> -->
  
  <!-- Website/Organization Structured Data -->
  <!-- <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script> -->
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <!-- <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list"> -->
        <!-- TODO: Replace with your lab's related works -->
        <!-- <a href="https://arxiv.org/abs/PAPER_ID_1" class="work-item" target="_blank"> -->
          <!-- <div class="work-info"> -->
            <!-- TODO: Replace with actual paper title -->
            <!-- <h5>Paper Title 1</h5> -->
            <!-- TODO: Replace with brief description -->
            <!-- <p>Brief description of the work and its main contribution.</p> -->
            <!-- TODO: Replace with venue and year -->
            <!-- <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a> -->
        <!-- TODO: Add more related works or remove extra items -->
        <!-- <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 2</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div> -->

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
          <!-- TODO: Replace with your paper title -->
          <h1 class="title is-1 publication-title">3D Human Mesh Modeling and Recovery from RGB and LiDAR</h1>
          
          <div class="is-size-5 publication-authors">
            <!-- TODO: Replace with your paper authors and their personal links -->
            <span class="author-block">
              <a href="https://rbregier.github.io/" target="_blank">Romain Brégier</a><sup>*</sup>,</span>
              <span class="author-block">
              <a href="https://istvansarandi.com/" target="_blank">István Sárándi</a><sup>*</sup>,</span>
              <span class="author-block">
              <a href="https://salmag98.github.io/" target="_blank">Salma Galaaoui</a><sup>*</sup>,</span>
              </span>
          </div>

          <img src="static/images/teaser.jpg" alt="Teaser image showcasing the topics of the tutorial" loading="lazy"/>

          <div class="is-size-5 publication-authors">
            <!-- TODO: Replace with your institution and conference/journal info -->
            <span class="author-block">CVPR 2026 Tutorial<br>Denver, Colorado</span>
            <!-- TODO: Remove this line if no equal contribution -->
            <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="section-links">

              <span class="link-block">
                <a href="#general-info"
                class="button is-normal is-rounded is-dark">
                <span>General Info</span>
              </a>
              </span>

              <span class="link-block">
                <a href="#schedule"
                class="button is-normal is-rounded is-dark">
                <span>Schedule</span>
              </a>
              </span>

              <span class="link-block">
                <a href="#organizers"
                class="button is-normal is-rounded is-dark">
                <span>Organizers</span>
              </a>
              </span>

              <span class="link-block">
                <a href="#program"
                class="button is-normal is-rounded is-dark">
                <span>Program</span>
              </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="general-info">
  <div class="container is-max-desktop content">
    <h2 class="title">General Information</h2>
    <div class="content has-text-justified">
    <p>In this half-day tutorial, we will provide a general overview of the state of the art in human mesh modeling and recovery.
We will begin with an introduction to <em>parametric human body mesh models</em> covering their motivation, design choices and trade-offs.
We will then move to an overview of <em>3D Human Mesh Recovery (HMR) from RGB</em> data,
before moving on to <em>HMR from depth data</em>, with a particular focus on challenging in-the-wild conditions and LiDAR data.</p>
    <ul>
      <li>Date: to be confirmed (2026, June 3 - 4)</li>
      <li>Format: half-day tutorial. Recordings will be made available online.</li>
      <li>Target audience: academic and industrial researchers of all levels. We will introduce foundational concepts and will step-by-step build toward more advanced topics throughout the presentations.</li>
      <li>Subject areas: Humans; Face, body, pose, gesture, movement (primary), 3D from single images, 3D from multi-view and sensors, Robot perception, Autonomous driving.</li>
    </ul>
    </div>
  </div>
</section>


<section class="section" id="motivation">
  <div class="container is-max-desktop content">
    <h2 class="title">Motivation</h2>
    <div class="content has-text-justified">
      <p>The understanding of human pose and shape is the cornerstone of multiple AI applications ranging from monitoring, AR/VR, sport and posture analysis, human-robot interaction all the way to autonomous driving.
Accurate human perception enables digital systems to interact appropriately with people in both indoor and outdoor environments.</p>

<p>Recent advances have pushed the field forward: modern methods now begin to achieve strong in-the-wild Human Mesh Recovery (HMR) performance, making them more reliable and useful for a wide variety of downstream tasks.
With this growing interest, the community has seen the emergence of datasets and shape-recovery models, as well as an expanding range of input modalities; including RGB, depth, LiDAR, etc.
At the same time, multiple human body models are being developed, each offering different levels of detail, interpretability and expressivity.</p>

<p>While these developments open up exciting new opportunities, they also introduce new challenges.
Designing and deploying human mesh recovery systems remains difficult due to dependency on the chosen body model, peculiarities of single-person and multi-person settings, challenges of occlusions and interactions with the 3D scene, and the reliance on data-hungry training pipelines.</p>

<p>This tutorial is therefore motivated by the need for a clear, structured, and accessible overview of the current HMR landscape.
The increasing use of foundation models and large-scale pretrained systems makes it particularly timely to disseminate a clear picture of the underlying principles of human body modeling and HMR, so that these methods can be more easily adopted, extended, and applied to adjacent fields beyond core human pose estimation.
Our goal is to lower the entry barrier for newcomers, provide a unifying perspective for practitioners, and foster collaboration between communities working on human modeling, 3D vision, graphics, and embodied AI.
By providing access to these concepts, we aim to maximize the impact of recent advances and encourage their use in downstream applications.</p>

  </div>
</section>


<section class="section" id="schedule">
  <div class="container is-max-desktop content">
    <h2 class="title">Tentative Schedule</h2>
    <div class="content has-text-justified">
      <table class="table table-striped">
        <tbody>
          <tr>
            <td width="150" style="background-color:#EFECCA">15 min</td>
            <td><b>Opening remarks and motivation</b></td>
          </tr>
          <tr>
            <td style="background-color:#26408B;color:#EFECCA">60 min</td>
            <td><b>Talk 1: A Hands-on tour of human parametric body mesh models (Romain Brégier)</b>
              <ul>
                <li>Motivation: what does it mean to model humans.</li>
                <li>tutorial on building a parametric body mesh model.</li>
                <li>State-of-the-art body models: specificities, use cases and interoperability.</li>
                <li>Applications (model registration, Gaussian avatars, physics simulation, HMR).</li>
              </ul>
            </td>
          </tr>
          <tr>
            <td style="background-color:#26408B;color:#EFECCA">60 min</td>
            <td><b>Talk 2: 3D human mesh recovery from images (István Sárándi)</b>
              <ul>
                <li>Motivation, broader context, relation to other tasks.</li>
                <li>Overview of core single-person HMR methods.</li>
                <li>Extensions: multi-person, video, world-space, multi-view, scene geometry.</li>
                <li>available datasets and their tradeoffs.</li>
                <li>Practical survey of state-of-the-art open-source tools.</li>
                <li>Open challenges, promising direction</li>
              </ul>
            </td>
          </tr>
          <tr>
            <td style="background-color:#A6CFD5;color:#EFECCA">15 min</td>
            <td><b>Coffee break with interactive demos</b></td>
          </tr>
          <tr>
            <td style="background-color:#26408B;color:#EFECCA">60 min</td>
            <td><b>Talk 3: 3D human pose and shape estimation from LiDAR point clouds (Salma Galaaoui)</b>
              <ul>
                <li>Active sensors and depth cameras.</li>
                <li>HMR in outdoor safety critical scenarios: motivating the use of LiDAR.</li>
                <li>Review of 3D HMR from depth and LiDAR.</li>
                <li>Existing datasets and metrics.</li>
                <li>Open research directions.</li>
                <li>Code examples and demo.</li>
              </ul>
            </td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
</section>

<section class="section" id="program">
  <div class="container is-max-desktop content">
    <h2 class="title">Program</h2>
    <div class="content has-text-justified">
      
    <h3 class="title is-3">A Hands-on Tour of Human Parametric Body Mesh Models</h3>

    <p>Parametric body models enable to encode the morphology and pose of humans in a compact, structured and controllable manner.
They have a long history in computer vision and graphics, and are central to computer animation, motion capture, human avatars, and Human Mesh Recovery.
Recently, body model parameters have also proven to be useful as a body language for Large Language Models (ChatPose).
Multiple body models have been proposed over the years -- such as the seminal SMPL -- and the release of models with permissive licenses such as MHR and Anny is likely to increase their adoption beyond academic research.
Yet, parametric body models are often perceived as complex black boxes by CV and ML practitioners unfamiliar with 3D geometry and computer graphics.</p>

  <p>We propose in this tutorial to change that.
  We will provide an overview of the field and help the audience navigate the zoo of parametric body models available (including SMPL, SMPL-H, SMPL-X, GHUM, STAR, SKEL, ATLAS, MHR, Anny, etc) and their targeted use cases.
  Most importantly, the tutorial will cover the underlying principles of parametric body mesh modeling such as rigid transformations, skeletal animation, direct and inverse kinematics, mesh representation, skinning, as well as shape modeling. We will provide multiple interactive examples with code to help the audience become familiar with with these topics. We will also share some practical insights regarding the use of parametric body models for tasks such as model fitting and human mesh recovery.</p>

    <h3 class="title is-3">3D Human Mesh Recovery from Images: A Practical Overview</h3>

<p>The field of 3D human mesh recovery from RGB images has witnessed remarkable progress in recent years. In the nearly five years since the ``SMPL Made Simple'' tutorial at CVPR 2021, the landscape has transformed. Methods have matured and real-time end-to-end models are ready for use in novel applications in an off-the-shelf manner. Despite this progress, adoption outside the core research community remains limited due to unfamiliarity and a perceived complexity of handling 3D geometry, leading many researchers to rely on legacy 2D pose estimation tools, unaware that modern HMR methods now offer full 3D body geometry with shape, contact surfaces, and metric scale at comparable computational cost. Our goal is therefore to make these methods more accessible to broader audiences.</p>

<p>This section provides a comprehensive overview of methods for recovering 3D human body meshes from RGB images. We set the stage by clarifying the task landscape, distinguishing HMR from related human-centric tasks such as 2D/3D pose estimation, DensePose, and avatar reconstruction, clarifying what the benefits of structured mesh recovery are (template correspondence, priors, contact modeling) and how it can be useful for adjacent research fields.
We trace the evolution of methods from early, time-consuming optimization-based model fitting (e.g., SMPLify) to modern real-time end-to-end approaches (HMR2.0, SMPLer-X, NLF) and non-real time high-accuracy models (SAM 3D Body), discussing both parametric and point-based output representation. Beyond single-image methods, we briefly touch on extensions to multi-person scenarios (Multi-HMR, AiOS), temporal tracking (4DHumans), world-space trajectory estimation (WHAM, SLAHMR, TRAM), multi-view settings, and integration with recent geometric foundation models (Human3R, HAMSt3R).
Through a practical survey of the state-of-the-art we offer recommendations for practitioners on choosing methods and avoiding common pitfalls. We conclude with open challenges and a live demonstration of human mesh recovery with the real-time SOTA NLF model.</p>


<h3 class="title is-3">3D Human Pose and Shape Estimation from LiDAR point-clouds: an Overview</h3>

<p>While RGB images offer high-resolution texture and color details that enhance object discrimination in perception systems, they remain highly sensitive to low-light conditions, glare, and overexposure, often yielding unusable data in challenging outdoor scenarios. Additionally, they suffer from inherent depth and scale ambiguity due to 3D-to-2D projection. To enhance HMR accuracy, researchers have integrated 3D data at the acquisition stage, first adopting Motion Capture (MoCap) systems and depth cameras (such as stereo-vision, structured-light, and time-of-flight sensors) in a Multi-Sensor-Fusion approach.</p>

<p>Indeed, HMR from depth (e.g., Kinect) is now considered a fairly mature research field, as this additional modality provides pixel-wise distances that enhance the overall accuracy of the system. Despite these advances, depth cameras remain sensitive to lighting conditions and unreliable in outdoor settings.</p>
<p>In comparison to RGB/depth cameras, LiDAR sensors offer a wider distance range, perform reliably under adverse weather conditions and are not sensitive to the absence of light which is crucial for safety critical applications such as autonomous driving, surveillance systems, etc. However, LiDAR point clouds present several challenges, notably, the lack of color and texture information, self- and mutual- occlusions, motion artifacts, noise-ridden measurements etc. Furthermore, due to the wider field of view of LiDAR scenes, the human subject sometimes represents a very small portion of the scene and can be described by a few points only depending on its distance to the sensor. Another notable issue is the high variability of scans from one LiDAR sensor to another that hinders the generalization capability of models. For these reasons, successful RGB-based methods and architectures rarely transfer seamlessly to LiDAR inputs, requiring adaptations for sensor-specific traits.</p>

<p>In this portion of the tutorial, we aim to provide the audience with an overview of HMR methods that integrate LiDAR information into their pipelines based on our <a href="https://arxiv.org/pdf/2509.12197">recent survey in the field</a>. We will introduce the different LiDAR technologies used particularly in outdoor settings along with a brief taxonomy. Then we dive into the current 3D HMR approaches from LiDAR, categorize them, introduce the additional metrics suitable for this modality and the common datasets and benchmarks in the field, and finish with some potential research directions and open questions. 
We will illustrate our presentation with practical examples of synthetic LiDAR point cloud generation, and we intend to demonstrate particularities of LiDAR point clouds -- which the general CVPR audience may not be familiar with -- through live demonstrations.</p>


</section>

<section class="section" id="related">
  <div class="container is-max-desktop content">
    <h2 class="title">Relation to Prior Tutorials</h2>
    <div class="content has-text-justified">
      <p>The most recent tutorial on body models and HMR was <a href="https://smpl.is.tue.mpg.de/">SMPL Made Simple</a> at CVPR 2021, and much has changed since then. Our tutorial provides a comprehensive, up-to-date introduction to make this research accessible to a broader audience.</p>
  
<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body"> -->
      <!-- TODO: Replace with your teaser video -->
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata"> -->
        <!-- TODO: Add your video file path here -->
        <!-- <source src="static/videos/banner_video.mp4" type="video/mp4"> -->
      <!-- </video> -->
      <!-- TODO: Replace with your video description -->
      <!-- <h2 class="subtitle has-text-centered"> -->
        <!-- Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.  -->
      <!-- </h2> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified"> -->
          <!-- TODO: Replace with your paper abstract -->
          <!-- <p>
            Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin ullamcorper tellus sed ante aliquam tempus. Etiam porttitor urna feugiat nibh elementum, et tempor dolor mattis. Donec accumsan enim augue, a vulputate nisi sodales sit amet. Proin bibendum ex eget mauris cursus euismod nec et nibh. Maecenas ac gravida ante, nec cursus dui. Vivamus purus nibh, placerat ac purus eget, sagittis vestibulum metus. Sed vestibulum bibendum lectus gravida commodo. Pellentesque auctor leo vitae sagittis suscipit.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End paper abstract -->


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item"> 
        <img src="static/images/teaser.jpg" alt="Teaser image showcasing the topics of the tutorial" loading="lazy"/> -->
        <!-- TODO: Replace with description of this result -->
        <!-- <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel2.jpg" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel3.jpg" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- TODO: Replace with your YouTube video ID -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1"> -->
          <!-- TODO: Add poster image for better preview -->
          <!-- <video poster="" id="video1" controls muted loop height="100%" preload="metadata"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4" type="video/mp4">
          </video>
        </div> -->
        <!-- <div class="item item-video2"> -->
          <!-- TODO: Add poster image for better preview -->
          <!-- <video poster="" id="video2" controls muted loop height="100%" preload="metadata"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3"> -->
          <!-- TODO: Add poster image for better preview -->
          <!-- <video poster="" id="video3" controls muted loop height="100%" preload="metadata"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->

<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2> -->

      <!-- TODO: Replace with your poster PDF -->
      <!-- <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->

  <section class="section" id="organizers">
    <div class="container is-max-desktop content">
      <h2 class="title" id="organizers">Organizers</h2>
        <div class="columns is-multiline is-centered is-variable is-0">

          <div class="column is-one-quarter">
            <a href="https://rbregier.github.io/">
              <div class="card">
                <div class="card-image">
                  <figure class="image">
                    <img class="is-rounded" src="./static/images/romain.jpg" alt="Romain Brégier">
                  </figure>
                </div>
                <div class="card-content">
                  <div class="media">
                    <div class="media-content" style="overflow-x: unset;">
                      <p class="title is-6 is-spaced">Romain Brégier</p>
                      <p class="subtitle is-7">NAVER LABS Europe</p>
                    </div>
                  </div>
                </div>
              </div>
            </a>
          </div>

          <div class="column is-one-quarter">
            <a href="https://salmag98.github.io">
              <div class="card">
                <div class="card-image">
                  <figure class="image">
                    <img class="is-rounded" src="./static/images/salma.jpg" alt="Salma Galaaoui">
                  </figure>
                </div>
                <div class="card-content">
                  <div class="media">
                    <div class="media-content" style="overflow-x: unset;">
                      <p class="title is-6 is-spaced">Salma Galaaoui</p>
                      <p class="subtitle is-7">valeo.ai | IMAGINE Lab ENPC</p>
                    </div>
                  </div>
                </div>
              </div>
            </a>
          </div>

          <div class="column is-one-quarter">
            <a href="https://istvansarandi.com/">
              <div class="card">
                <div class="card-image">
                  <figure class="image">
                    <img class="is-rounded" src="./static/images/istvan.jpg" alt="István Sárándi">
                  </figure>
                </div>
                <div class="card-content">
                  <div class="media">
                    <div class="media-content" style="overflow-x: unset;">
                      <p class="title is-6 is-spaced">István Sárándi</p>
                      <p class="subtitle is-7">University of Tubingen</p>
                    </div>
                  </div>
                </div>
              </div>
            </a>
          </div>
        </div>
        
        <div class="columns is-multiline is-centered is-variable is-0">

          <div class="column is-one-quarter">
            <a href="https://fabienbaradel.github.io/">
              <div class="card">
                <div class="card-image">
                  <figure class="image">
                    <img class="is-rounded" src="./static/images/fabien.jpg" alt="Fabien Baradel">
                  </figure>
                </div>
                <div class="card-content">
                  <div class="media">
                    <div class="media-content" style="overflow-x: unset;">
                      <p class="title is-6 is-spaced">Fabien Baradel</p>
                      <p class="subtitle is-7">NAVER LABS Europe</p>
                    </div>
                  </div>
                </div>
              </div>
            </a>
          </div>

          <div class="column is-one-quarter">
            <a href="https://nerminsamet.com/">
              <div class="card">
                <div class="card-image">
                  <figure class="image">
                    <img class="is-rounded" src="./static/images/nermin.jpg" alt="Nermin Samet">
                  </figure>
                </div>
                <div class="card-content">
                  <div class="media">
                    <div class="media-content" style="overflow-x: unset;">
                      <p class="title is-6 is-spaced">Nermin Samet</p>
                      <p class="subtitle is-7">valeo.ai</p>
                    </div>
                  </div>
                </div>
              </div>
            </a>
          </div>

          <div class="column is-one-quarter">
            <a href="https://davidpicard.github.io">
              <div class="card">
                <div class="card-image">
                  <figure class="image">
                    <img class="is-rounded" src="./static/images/david.jpg" alt="David Picard">
                  </figure>
                </div>
                <div class="card-content">
                  <div class="media">
                    <div class="media-content" style="overflow-x: unset;">
                      <p class="title is-6 is-spaced">David Picard</p>
                      <p class="subtitle is-7">IMAGINE Lab ENPC</p>
                    </div>
                  </div>
                </div>
              </div>
            </a>
          </div>

        </div>
    </div>
  </section>


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{YourPaperKey2024,
  title={Your Paper Title Here},
  author={First Author and Second Author and Third Author},
  journal={Conference/Journal Name},
  year={2024},
  url={https://your-domain.com/your-project-page}
}</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            Contact: <a href="mailto:human-mesh-tutorial@googlegroups.com">human-mesh-tutorial@googlegroups.com</a>
            <br><br>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
